{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ef9e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ec117998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras import datasets, layers\n",
    "from keras import models, losses\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing.sequence import make_sampling_table\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "# Launch tensorboard session\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4aa4e509",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "SEQUENCE_LENGTH = 4\n",
    "# Set the number of negative samples per positive context.\n",
    "NUM_NS = 4\n",
    "WINDOW_SIZE = 2\n",
    "SEED = 42\n",
    "EMBEDDING_DIM = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de322c1f",
   "metadata": {},
   "source": [
    "## 2.1 The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1612b2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Book of Moses:  Called Genesis\n",
      "\n",
      "\n",
      "1:1 In the beginning God created the heaven and the earth.\n",
      "\n",
      "1:2 And the earth was without form, and void; and darkness was upon\n",
      "the face of the deep. And the Spirit of God moved upon the face of the\n",
      "waters.\n",
      "\n",
      "1:3 And God said, Let there be light: and there was light.\n",
      "\n",
      "1:4 And God saw the light, that it was good: and God divided the light\n",
      "from the darkness.\n",
      "\n",
      "1:5 And God called the light Day, and the darkness he called Night.\n",
      "And the evening and the morning were the first day.\n",
      "\n",
      "1:6 And God said, Let there be a firmament in the midst of the waters,\n",
      "and let it divide the waters from the waters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading data \n",
    "file_path = '../Homework10/bible.txt'\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    text = f.read().splitlines()\n",
    "\n",
    "for line in text[:20]:\n",
    "    print(line) \n",
    "\n",
    "# Preparing Data for Model\n",
    "text_ds = tf.data.TextLineDataset(file_path).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9ec2bb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72192 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocabulary = dict((x, i) for i, x in enumerate(np.unique(list(text))))\n",
    "vocab_size = len(vocabulary)\n",
    "print(vocab_size, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a582a7ea",
   "metadata": {},
   "source": [
    "## 2.2 Word Embeddings\n",
    "    Take care of the following:\n",
    "    • Convert to lower case, remove new-line characters and special characters\n",
    "    • Tokenize the string into word-tokens by using one of the word-level tokenizers from tensorflow-text e.g. this one.\n",
    "    • For performance purposes, we advise that you work with only a subset of all the words that are in corpus. We recommend starting out with only the 10000 most common words 3.\n",
    "    Next, you need to create the input-target pairs, including a word (input) and a context word (target). We suggest a context window of (but if you want to go bigger and have the computational resources to spare, go for it). Let’s take the sentence ”The cat climbed the tree” for instance. For the input word ”climbed” we want to predict ”the, cat, the, tree”. The resulting input-target pairs to feed to the network will be (climbed, the), (climbed, cat), (climbed, the), (climbed, tree). Note that you leave out the index 0 when creating the pairs, so there is no pair (climbed, climbed). Create a data set from these pairs and batch and shuffle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c41c872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize sentences from the corpus\n",
    "# Now, create a custom standardization function to lowercase the text and\n",
    "# remove special characters and punctuation.\n",
    "def custom_standardization(input_data: str):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    # lowercase = lowercase.split()\n",
    "    return tf.strings.regex_replace(lowercase,\n",
    "                                    '[%s]' % re.escape(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b0d48e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fadff85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'and', 'of', 'to', 'that', 'in', 'he', 'shall', 'unto', 'for', 'i', 'his', 'a', 'lord', 'they', 'be', 'is', 'him'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the vocabulary for text dataset\n",
    "vectorize_layer.adapt(text_ds.batch(BATCH_SIZE))\n",
    "# Returns a list of all vocabulary tokens sorted (descending) by their frequency\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "60380b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a79c5765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74644 \n",
      "\n",
      "[  2 219 404   4] => ['the', 'first', 'book', 'of']\n",
      "[1003    7    2  684] => ['11', 'in', 'the', 'beginning']\n",
      "[1002    3    2  111] => ['12', 'and', 'the', 'earth']\n",
      "[  2 230   4   2] => ['the', 'face', 'of', 'the']\n",
      "[302   0   0   0] => ['waters', '', '', '']\n",
      "[1000    3   28   32] => ['13', 'and', 'god', 'said']\n",
      "[999   3  28 181] => ['14', 'and', 'god', 'saw']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Flatten the dataset into a list of sentence vector sequences\n",
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences), '\\n')\n",
    "# Inspect a few examples from sequences\n",
    "for seq in sequences[:7]:\n",
    "    print(f'{seq} => {[inverse_vocab[i] for i in seq]}')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ed4117da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, \n",
    "                            vocab_size, seed):\n",
    "    # Elements of each training example are appended to these lists.\n",
    "    targets, contexts, labels = [], [], []\n",
    "    # Build the sampling table for 'vocab_size' tokens.\n",
    "    sampling_table = make_sampling_table(vocab_size)\n",
    "\n",
    "    # Iterate over all sequences (sentences) in the dataset.\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "        # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "        positive_skip_grams, _ = skipgrams(\n",
    "            sequence,\n",
    "            vocabulary_size=vocab_size,\n",
    "            sampling_table=sampling_table,\n",
    "            window_size=window_size, \n",
    "            negative_samples=0\n",
    "        )        \n",
    "\n",
    "        # Iterate over each positive skip-gram pair to produce training examples\n",
    "        # with a positive context word and negative samples.\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "            context_class = tf.expand_dims(tf.constant([context_word], dtype='int64'), 1)\n",
    "            negative_sample_candidate, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                true_classes=context_class,\n",
    "                num_true=1,\n",
    "                num_sampled=num_ns,\n",
    "                unique=True,\n",
    "                range_max=vocab_size,\n",
    "                seed=seed,\n",
    "                name='negative_sampling'\n",
    "            )\n",
    "        \n",
    "            # Build context and label vectors (for one target word)\n",
    "            context = tf.concat([tf.squeeze(context_class, 1), negative_sample_candidate], 0)\n",
    "            label = tf.constant([1] + [0] * num_ns, dtype='int64')\n",
    "\n",
    "            # Append each element from the training example to global lists.\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "\n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d5a6c350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 74644/74644 [04:13<00:00, 294.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate training examples from sequences\n",
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    num_ns=NUM_NS,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)\n",
    "    \n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e3e03657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets shape: (98085,)\n",
      "contexts shape: (98085, 5)\n",
      "labels shape: (98085, 5)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print(f'targets shape: {targets.shape}')\n",
    "print(f'contexts shape: {contexts.shape}')\n",
    "print(f'labels shape: {labels.shape}')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4c6635c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the dataset for performance\n",
    "def config_dataset(targets, contexts, labels, \n",
    "                    buffer_size, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "    dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
    "    # Apply Dataset.cache and Dataset.prefetch to improve performance:\n",
    "    dataset = dataset.cache().prefetch(buffer_size = tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "534bdaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "dataset = config_dataset(targets, contexts, labels, BUFFER_SIZE, BATCH_SIZE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1540fd",
   "metadata": {},
   "source": [
    "## 2.3 The model\n",
    "    Implement a SkipGram model to create the word embeddings. There are multiple ways of implementing a Skip Gram in TensorFlow.\n",
    "    \n",
    "    Subclassed word2vec model\n",
    "    Use the Keras Subclassing API to define my Skip model with the following layers:\n",
    "    • target_embedding: A tf.keras.layers.Embedding layer, which looks up the embedding of a word when it appears as a target word. The number of parameters in this layer are (vocab_size * embedding_dim).\n",
    "    • context_embedding: Another tf.keras.layers.Embedding layer, which looks up the embedding of a word when it appears as a context word. The number of parameters in this layer are the same as those in target_embedding, i.e. (vocab_size * embedding_dim).\n",
    "    • dots: A tf.keras.layers.Dot layer that computes the dot product of target and context embeddings from a training pair.\n",
    "    • flatten: A tf.keras.layers.Flatten layer to flatten the results of dots layer into logits.\n",
    "    With the subclassed model, i can define the call() function that accepts (target, context) pairs which can then be passed into their corresponding embedding layer. Reshape the context_embedding to perform a dot product with target_embedding and return the flattened result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "311f942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Skip Gram model class\n",
    "class SkipGram(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_ns):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.target_embedding = layers.Embedding(\n",
    "            vocab_size, embedding_dim, input_length=1, name=\"w2v_embedding\")\n",
    "        self.context_embedding = layers.Embedding(\n",
    "            vocab_size, embedding_dim, input_length=num_ns)\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "        # context: (batch, context)\n",
    "        if len(target.shape) == 2:\n",
    "            target = tf.squeeze(target, axis=1)\n",
    "        # target: (batch,)\n",
    "        word_emb = self.target_embedding(target)\n",
    "        # word_emb: (batch, embed)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        # context_emb: (batch, context, embed)\n",
    "        dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "        # dots: (batch, context)\n",
    "        return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b571131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "def custom_loss(x_logit, y_true):\n",
    "    return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "04ec88c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_gram = SkipGram(vocab_size, EMBEDDING_DIM, NUM_NS)\n",
    "skip_gram.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "# Define a callback to log training statistics for TensorBoard\n",
    "EXPERIMENT_NAME = 'Word embedding'\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(f'./logs/{EXPERIMENT_NAME}/{current_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6691cce",
   "metadata": {},
   "source": [
    "## 2.4 Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "22c88e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "95/95 [==============================] - 13s 97ms/step - loss: 1.6070 - accuracy: 0.2741\n",
      "Epoch 2/20\n",
      "95/95 [==============================] - 9s 93ms/step - loss: 1.5567 - accuracy: 0.5956\n",
      "Epoch 3/20\n",
      "95/95 [==============================] - 9s 93ms/step - loss: 1.4121 - accuracy: 0.5934\n",
      "Epoch 4/20\n",
      "95/95 [==============================] - 9s 95ms/step - loss: 1.2664 - accuracy: 0.5922\n",
      "Epoch 5/20\n",
      "95/95 [==============================] - 10s 104ms/step - loss: 1.1612 - accuracy: 0.6098\n",
      "Epoch 6/20\n",
      "95/95 [==============================] - 11s 117ms/step - loss: 1.0765 - accuracy: 0.6362\n",
      "Epoch 7/20\n",
      "95/95 [==============================] - 19s 200ms/step - loss: 1.0003 - accuracy: 0.6642\n",
      "Epoch 8/20\n",
      "95/95 [==============================] - 19s 201ms/step - loss: 0.9288 - accuracy: 0.6938\n",
      "Epoch 9/20\n",
      "95/95 [==============================] - 19s 200ms/step - loss: 0.8611 - accuracy: 0.7222\n",
      "Epoch 10/20\n",
      "95/95 [==============================] - 19s 201ms/step - loss: 0.7971 - accuracy: 0.7498\n",
      "Epoch 11/20\n",
      "95/95 [==============================] - 19s 200ms/step - loss: 0.7372 - accuracy: 0.7765\n",
      "Epoch 12/20\n",
      "95/95 [==============================] - 19s 200ms/step - loss: 0.6814 - accuracy: 0.8001\n",
      "Epoch 13/20\n",
      "95/95 [==============================] - 19s 201ms/step - loss: 0.6299 - accuracy: 0.8206\n",
      "Epoch 14/20\n",
      "95/95 [==============================] - 19s 201ms/step - loss: 0.5825 - accuracy: 0.8391\n",
      "Epoch 15/20\n",
      "95/95 [==============================] - 19s 202ms/step - loss: 0.5390 - accuracy: 0.8549\n",
      "Epoch 16/20\n",
      "95/95 [==============================] - 19s 201ms/step - loss: 0.4993 - accuracy: 0.8691\n",
      "Epoch 17/20\n",
      "95/95 [==============================] - 19s 201ms/step - loss: 0.4630 - accuracy: 0.8809\n",
      "Epoch 18/20\n",
      "95/95 [==============================] - 19s 201ms/step - loss: 0.4299 - accuracy: 0.8918\n",
      "Epoch 19/20\n",
      "95/95 [==============================] - 19s 201ms/step - loss: 0.3999 - accuracy: 0.9018\n",
      "Epoch 20/20\n",
      "95/95 [==============================] - 20s 206ms/step - loss: 0.3725 - accuracy: 0.9102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2045ca678b0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "skip_gram.fit(\n",
    "        dataset, \n",
    "        epochs = 20,\n",
    "        callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ee4724b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"skip_gram\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " w2v_embedding (Embedding)   multiple                  4620288   \n",
      "                                                                 \n",
      " embedding (Embedding)       multiple                  4620288   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,240,576\n",
      "Trainable params: 9,240,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at a summary of Skip Gram model\n",
    "skip_gram.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bdc5941e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6d059372c7e97eef\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6d059372c7e97eef\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e6ea2a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the weights from the model using Model.get_layer and Layer.get_weights.\n",
    "weights = skip_gram.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a64df0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save the vectors and metadata files\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index == 0:\n",
    "        continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a91c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297daddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
