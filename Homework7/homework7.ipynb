{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c182b257",
   "metadata": {},
   "source": [
    "## 2.1 Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fc4749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29ea4e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "train_ds, test_ds = tfds.load('mnist', split=['train','test'], as_supervised=True)\n",
    "\n",
    "def cumsum_dataset(ds, seq_len):\n",
    "    #only get the targets, to keep this demonstration simple (and force students to understand the code if they are using it by rewriting it respectively)\n",
    "    ds = ds.map(lambda x, t: tf.cast(t, dtype=tf.dtypes.int32))\n",
    "    # use window to create subsequences. This means ds is not a dataset of datasets, i.e. every single entry in the dataset is itself a small tf.data.Dataset object with seq_len many entries!\n",
    "    ds = ds.window(seq_len)\n",
    "    #make sure to check tf.data.Dataset.scan() to understand how this works!\n",
    "    def alternating_scan_function(state, elem):\n",
    "        #state is allways the sign to use!\n",
    "        old_sign = state\n",
    "        #just flip the sign for every element\n",
    "        new_sign = old_sign*-1\n",
    "        #elem is just the target of the element. We need to apply the appropriate sign to it!\n",
    "        signed_target = elem*old_sign\n",
    "        #we need to return a tuple for the scan function: The new state and the output element\n",
    "        out_elem = signed_target\n",
    "        new_state = new_sign\n",
    "        return new_state, out_elem\n",
    "    #we now want to apply this function via scanning, resulting in a dataset where the signs are alternating\n",
    "    #remember we have a dataset, where each element is a sub dataset due to the windowing!\n",
    "    ds = ds.map(lambda sub_ds: sub_ds.scan(initial_state=1, scan_func=alternating_scan_function))\n",
    "    #now we need a scanning function which implements a cumulative sum, very similar to the cumsum used above\n",
    "    def scan_cum_sum_function(state, elem):\n",
    "        #state is the sum up the the current element, element is the new digit to add to it\n",
    "        sum_including_this_elem = state+elem\n",
    "        #both the element at this position and the returned state should just be sum up to this element, saved in sum_including_this_elem\n",
    "        return sum_including_this_elem, sum_including_this_elem\n",
    "    #again we want to apply this to the subdatasets via scan, with a starting state of 0 (sum before summing is zero...)\n",
    "    ds = ds.map(lambda sub_dataset: sub_dataset.scan(initial_state=0, scan_func=scan_cum_sum_function))\n",
    "    #finally we need to create a single element from everything in the subdataset\n",
    "    ds = ds.map(lambda sub_dataset: sub_dataset.batch(seq_len).get_single_element())\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11ae9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mnist_dataset(mnist_ds, seq_len, ds_type):\n",
    "    # choose type of dataset\n",
    "    if ds_type == 'Train':\n",
    "        # flatten the images into vector\n",
    "        mnist_ds = mnist_ds.map(lambda img, target: (tf.reshape(img, (-1,28,28,1)), target))\n",
    "        # convert data from uint8 to float32\n",
    "        mnist_ds = mnist_ds.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "        # Sloppy input normalization, just bringing target values from range [0, 255] to [0, 1]\n",
    "        mnist_ds = mnist_ds.map(lambda target, img: ((target/128.), img))\n",
    "        mnist_ds.apply(lambda dataset: cumsum_dataset(dataset, seq_len)).take(10)\n",
    "        # shuffle, batch, prefetch\n",
    "        mnist_ds = mnist_ds.shuffle(1000)\n",
    "        mnist_ds = mnist_ds.batch(32)\n",
    "        mnist_ds = mnist_ds.prefetch(tf.data.AUTOTUNE)\n",
    "        # return preprocessed dataset\n",
    "        return mnist_ds\n",
    "    elif ds_type == 'Test':\n",
    "        # flatten the images into vector\n",
    "        mnist_ds = mnist_ds.map(lambda img, target: (tf.reshape(img, (-1,28,28,1)), target))\n",
    "        # convert data from uint8 to float32\n",
    "        mnist_ds = mnist_ds.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "        # Sloppy input normalization, just bringing target values from range [0, 255] to [0, 1]\n",
    "        mnist_ds = mnist_ds.map(lambda target, img: ((target/128.), img))\n",
    "        mnist_ds.apply(lambda dataset: cumsum_dataset(dataset, seq_len)).take(10)\n",
    "        # batch, prefetch\n",
    "        mnist_ds = mnist_ds.batch(32)\n",
    "        mnist_ds = mnist_ds.prefetch(tf.data.AUTOTUNE)\n",
    "        # return preprocessed dataset\n",
    "        return mnist_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c5be67",
   "metadata": {},
   "source": [
    "## 2.2 The CNN & LSTM Network\n",
    "    The first part of your model should have a basic CNN structure. This part should extract vector representations from each MNIST image using Conv2D layers as well as (global) pooling or Flatten layers. A Conv2D layer can be called on a batch of sequences of images, where the time dimension is in the second axis. The time dimension will then be processed like a second batch dimension (search for ”extended batch shape” in the Conv2D layer documentation page for an example).\n",
    "    While Conv2D layers accept our (batch, sequence-length, image) data structure with their extended batch size functionality, for the pooling layers to work correctly you will have to wrap them in TensorFlow’s TimeDistributed layers.\n",
    "    Once you have encoded all images as vectors, the shape of the tensor should be (batch, sequence-length, features), which can be fed to a non-convolutional standard LSTM.\n",
    "    \n",
    "## 2.3 LSTM AbstractRNNCell layer\n",
    "    For the LSTM, we want to not use the (optimized) keras implementation, but instead we want you to be able to implement the LSTM logic that is applied at each time-step yourselves. For this, we want to subclass the AbstractRNNCell layer and implement its methods and define the required properties. Those are state size, output size, and get initial state, which determines the initial hidden and cell state of the LSTM (usually tensors filled with zeros). \n",
    "    The LSTM-cell layer’s call method should take one (batch of) feature vector(s) as its input, along with the ”states”, a list containing the different state tensors of the LSTM cell (cell state and hidden state!).\n",
    "    In the call method, the layer then uses these two states together with thevector representation of the current MNIST image in the sequence, and updates both the hidden state, and the cell state (in the way that it is done in an LSTM). The returns should be the output of the LSTM, to be used to compute the model output for this time-step (usually the hidden state), as well as a list containing the new states (e.g. [new hidden state, new cell state]).\n",
    "\n",
    "## 2.4 Wrapping the LSTM-Cell layer with an RNN layer\n",
    "    Since the LSTM cell only provides the computation for one time-step, you would need to write a wrapper layer around it, that applies it to every time-step in the sequence, aggregating the outputs and states in a for loop.\n",
    "    In Tensorflow you should use the RNN layer for this, tf.keras.layers.RNN takes an instance of your LSTM cell as the first argument in its constructor. You also need to specify whether you want the RNN wrapper layer to return the output of your LSTM-cell for every time-step or only for the last step (with the argument return sequences=True). This is generally task-dependent, so think about what makes most sense in this case. For speed-ups (at the cost of memory usage) you can set the ”unroll” argument to True.\n",
    "    The ”wrapper” RNN layer then takes the sequence of vector representations of the mnist images as its input (batch, seq len, feature dim).\n",
    "\n",
    "## 2.5 Computing the model output\n",
    "    With the output of your RNN-wrapped LSTM-Cell, you can now compute the model predictions. Again depending on the task, you need to think about what your predictions should be (generally have one prediction per target that is associated with your sequence). Dense layers also behave in the same way as Conv2D layers - when there is an additional time-dimension (batch, time, features), they apply the same computation for every time-index and for every batch-index. So you could (if the task demands it) use the same Dense layer to predict targets for all time-steps. You likely do not want to have a Dense layer for each time-step’s target prediction (potential for overfitting!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4d74430",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(tf.keras.layers.AbstractRNNCell):\n",
    "    def __init__(self, recurrent_units_1, recurrent_units_2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.recurrent_units_1 = recurrent_units_1\n",
    "        self.recurrent_units_2 = recurrent_units_2\n",
    "\n",
    "        self.linear_1 = tf.keras.layers.TimeDistributed(\n",
    "            tf.keras.layers.Dense(recurrent_units_1))\n",
    "        self.linear_2 = tf.keras.layers.TimeDistributed(\n",
    "            tf.keras.layers.Dense(recurrent_units_2))\n",
    "    \n",
    "        # First recurrent layer in the RNN\n",
    "        self.recurrent_layer_1 = tf.keras.layers.Conv2D(filters=recurrent_units_1,\n",
    "                                    kernel_size=3,\n",
    "                                    padding = 'same',\n",
    "                                    kernel_initializer=tf.keras.initializers.Orthogonal(\n",
    "                                        gain=1.0, seed=None),\n",
    "                                    activation=tf.nn.tanh)\n",
    "        \n",
    "        # layer normalization for trainability\n",
    "        self.batch_norm_1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        # Second recurrent layer in the RNN\n",
    "        self.recurrent_layer_2 = tf.keras.layers.Conv2D(filters=recurrent_units_2,\n",
    "                                    kernel_size=3,\n",
    "                                    padding = 'same',\n",
    "                                    kernel_initializer=tf.keras.initializers.Orthogonal(\n",
    "                                        gain=1.0, seed=None),\n",
    "                                    activation=tf.nn.tanh)\n",
    "\n",
    "        # layer normalization for trainability\n",
    "        self.batch_norm_2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return [tf.TensorShape([self.recurrent_units_1]), \n",
    "                tf.TensorShape([self.recurrent_units_2])]\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return [tf.TensorShape([self.recurrent_units_2])]\n",
    "\n",
    "    def initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
    "        return [tf.zeros([self.recurrent_units_1]),\n",
    "                tf.zeros([self.recurrent_units_2])]\n",
    "    \n",
    "    @tf.function\n",
    "    def __call__(self, inputs, states):\n",
    "        # Unpack the states\n",
    "        state_layer_1 = states[0]\n",
    "        state_layer_2 = states[1]\n",
    "\n",
    "        # Linearly project input\n",
    "        x = self.linear_1(inputs) + state_layer_1\n",
    "        # Apply first reccurent layer\n",
    "        new_state_layer_1 = self.recurrent_layer_1(x)\n",
    "        # Apply first layer's layer norm\n",
    "        x = self.batch_norm_1(new_state_layer_1)\n",
    "        # linearly project output of layer norm\n",
    "        x = self.linear_2(x) + state_layer_2\n",
    "        # Apply second reccurent layer\n",
    "        new_state_layer_2 = self.recurrent_layer_2(x)\n",
    "        # Apply second layer's layer norm\n",
    "        x = self.batch_norm_2(new_state_layer_2)\n",
    "\n",
    "        # Return output and the list of new states of the layers\n",
    "        return x, [new_state_layer_1, new_state_layer_2] \n",
    "\n",
    "    def get_config(self):\n",
    "        return {'recurrent_units_1': self.recurrent_units_1,\n",
    "                'recurrent_units_2': self.recurrent_units_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccc42238",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(tf.keras.Model):\n",
    "    def __init__(self, input_n, output_n):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_cell = RNNCell(input_n, output_n)\n",
    "        # Return_sequences collects and returns the output of the rnn_cell for all time-steps\n",
    "        # Unroll unrolls the network for speed (at the cost of memory)\n",
    "        self.rnn_layer = tf.keras.layers.RNN(self.rnn_cell, return_sequences=False, unroll=True)\n",
    "\n",
    "        self.avg_global_pool = tf.keras.layers.TimeDistributed(\n",
    "            tf.keras.layers.GlobalAvgPool2D())\n",
    "\n",
    "        self.flatten = tf.keras.layers.TimeDistributed(\n",
    "            tf.keras.layers.Flatten())\n",
    "\n",
    "        self.output_layer = tf.keras.layers.TimeDistributed(\n",
    "            tf.keras.layers.Dense(units=10,activation=tf.nn.sigmoid))\n",
    "\n",
    "        self.metrics_list = [\n",
    "            tf.keras.metrics.Mean(name='loss'),\n",
    "            tf.keras.metrics.CategoricalAccuracy(name='accuracy')\n",
    "        ]\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, sequence, training=False):\n",
    "        x = self.rnn_layer(sequence)\n",
    "        x = self.avg_global_pool(sequence)\n",
    "        x = self.flatten(sequence)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_state()\n",
    "\n",
    "    def train_step(self, data):\n",
    "        sequence, label = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self(sequence, training=True)\n",
    "            loss = self.compiled_loss(label, output, regularization_losses=self.losses)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        \n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        self.metrics[0].update_state(loss)\n",
    "        self.metrics[1].update_state(label, output)\n",
    "        \n",
    "        return {m.name : m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        sequence, label = data\n",
    "        output = self(sequence, training=False)\n",
    "        loss = self.compiled_loss(label, output, regularization_losses=self.losses)\n",
    "                \n",
    "        self.metrics[0].update_state(loss)\n",
    "        self.metrics[1].update_state(label, output)\n",
    "        \n",
    "        return {m.name : m.result() for m in self.metrics}    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c38c7a",
   "metadata": {},
   "source": [
    "## 2.6 Training\n",
    "    We still highly recommend to write another custom training-loop for this home\u0002work. However, if you want to follow the code presented in this week, feel free to already use the model.compile and model.fit methods for the first time. Make sure to track your experiments properly, save configs (e.g. hyperparameters) of your settings, save logs (e.g. with Tensorboard) and checkpoint your model’s weights (or even the complete model). To visualize your results you can modify your training loop to also write metrics to lists, or rely on the default history callback that model.fit uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc1b35f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(history):\n",
    "    # Plotting the loss data\n",
    "    plt.plot(history.history['loss'], label='training')\n",
    "    plt.plot(history.history['val_loss'], label='validation')\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Categorical Crossentropy loss')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdfcf518",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0afd7324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1, 28, 28, 1) (32,)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = prepare_mnist_dataset(train_ds, SEQUENCE_LENGTH, 'Train')\n",
    "# Сheck the contents of the dataset\n",
    "for img, label in train_dataset:\n",
    "    print(img.shape, label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2324ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1, 28, 28, 1) (32,)\n"
     ]
    }
   ],
   "source": [
    "val_dataset = prepare_mnist_dataset(train_ds, SEQUENCE_LENGTH, 'Test')\n",
    "for img, label in val_dataset:\n",
    "    print(img.shape, label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6db34e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape = (28,28,1)\n",
    "input_units = 24\n",
    "output_units = 48\n",
    "\n",
    "model = RNNModel(input_units, output_units)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f809c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=optimizer,\n",
    "                loss=loss) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4361a8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\п\\AppData\\Local\\Temp\\ipykernel_4352\\4165944845.py\", line 43, in train_step\n        loss = self.compiled_loss(label, output, regularization_losses=self.losses)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None,) and (None, 1, 10) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4352\\3076286610.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Train the model with fit function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m history = model.fit(train_dataset,\n\u001b[0m\u001b[0;32m      7\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4352\\4165944845.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompiled_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularization_losses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\п\\AppData\\Local\\Temp\\ipykernel_4352\\4165944845.py\", line 43, in train_step\n        loss = self.compiled_loss(label, output, regularization_losses=self.losses)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None,) and (None, 1, 10) are incompatible\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME = 'RNN_cumsum'\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logging_callback = tf.keras.callbacks.TensorBoard(log_dir=f'./logs/{EXPERIMENT_NAME}/{current_time}')\n",
    "\n",
    "# Train the model with fit function\n",
    "history = model.fit(train_dataset,\n",
    "                        validation_data=val_dataset,\n",
    "                        epochs = 20,\n",
    "                        callbacks = logging_callback)\n",
    "    \n",
    "# Save model\n",
    "model.save(filepath='./saved_models/')\n",
    "\n",
    "# Visualize the data\n",
    "visualization(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ac60a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01713a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
