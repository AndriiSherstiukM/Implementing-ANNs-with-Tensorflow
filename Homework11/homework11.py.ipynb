{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ab881f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.layers import TextVectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0917460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "SEQUENCE_LENGTH = 4\n",
    "# Set the number of negative samples per positive context.\n",
    "NUM_HEADS = 2\n",
    "SEED = 42\n",
    "FEED_FORWARD_DIM = 64\n",
    "\n",
    "buffer_size = 5000\n",
    "embedding_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "92b7e8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Book of Moses:  Called Genesis\n",
      "\n",
      "\n",
      "1:1 In the beginning God created the heaven and the earth.\n",
      "\n",
      "1:2 And the earth was without form, and void; and darkness was upon\n",
      "the face of the deep. And the Spirit of God moved upon the face of the\n",
      "waters.\n",
      "\n",
      "1:3 And God said, Let there be light: and there was light.\n",
      "\n",
      "1:4 And God saw the light, that it was good: and God divided the light\n",
      "from the darkness.\n",
      "\n",
      "1:5 And God called the light Day, and the darkness he called Night.\n",
      "And the evening and the morning were the first day.\n",
      "\n",
      "1:6 And God said, Let there be a firmament in the midst of the waters,\n",
      "and let it divide the waters from the waters.\n",
      "\n",
      "1:7 And God made the firmament, and divided the waters which were\n",
      "under the firmament from the waters which were above the firmament:\n",
      "and it was so.\n",
      "\n",
      "1:8 And God called the firmament Heaven. And the evening and the\n",
      "morning were the second day.\n",
      "\n",
      "1:9 And God said, Let the waters under the heaven be gathered together\n",
      "unto one place, and let the dry land appear: and it was so.\n",
      "\n",
      "1:10 And God called the dry land Earth; and the gathering together of\n",
      "the waters called he Seas: and God saw that it was good.\n",
      "\n",
      "1:11 And God said, Let the earth bring forth grass, the herb yielding\n",
      "seed, and the fruit tree yielding fruit after his kind, whose seed is\n",
      "in itself, upon the earth: and it was so.\n",
      "\n",
      "1:12 And the earth brought forth grass, and herb yielding seed after\n",
      "his kind, and the tree yielding fruit, whose seed was in itself, after\n",
      "his kind: and God saw that it was good.\n",
      "\n",
      "1:13 And the evening and the morning were the third day.\n",
      "\n",
      "1:14 And God said, Let there be lights in the firmament of the heaven\n",
      "to divide the day from the night; and let them be for signs, and for\n",
      "seasons, and for days, and years: 1:15 And let them be for lights in\n",
      "the firmament of the heaven to give light upon the earth: and it was\n",
      "so.\n",
      "\n",
      "1:16 And God made two great lights; the greater light to rule the day,\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "file_path = '../Homework10/bible.txt'\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    text = f.read().splitlines()\n",
    "\n",
    "for line in text[:50]:\n",
    "    print(line) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "40bf1d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72192 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preparing Data for Model\n",
    "vocabulary = dict((x, i) for i, x in enumerate(np.unique(list(text))))\n",
    "vocab_size = len(vocabulary)\n",
    "print(vocab_size, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a23c34a",
   "metadata": {},
   "source": [
    "## 2.1 The dataset, preprocessing & tokenizatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "932aef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom standardization function to lowercase the text and\n",
    "# remove special characters and punctuation.\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    # lowercase = lowercase.split()\n",
    "    return tf.strings.regex_replace(lowercase,\n",
    "                                    '[%s]' % re.escape(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7d5d5b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_labels(data):\n",
    "    \"\"\"\n",
    "    Shift word sequences by 1 position so that the target for position (i) is\n",
    "    word at position (i+1). The model will use all words up till position (i)\n",
    "    to predict the next word.\n",
    "    \"\"\"\n",
    "\n",
    "    data = tf.expand_dims(data, axis=-1)\n",
    "    tokenized_sentences = vectorize_layer(data)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "15ec9f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Data for Model\n",
    "dataset = tf.data.TextLineDataset(file_path).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f868636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the dataset for performance\n",
    "def config_dataset(text_ds, buffer_size, batch_size):\n",
    "    text_ds = text_ds.shuffle(buffer_size=32)\n",
    "    text_ds = text_ds.batch(batch_size, drop_remainder=True)\n",
    "    text_ds = text_ds.map(prepare_input_labels)\n",
    "    # Apply Dataset.cache and Dataset.prefetch to improve performance:\n",
    "    text_ds = text_ds.cache()\n",
    "    text_ds = text_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return text_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e0b089f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size - 1,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQUENCE_LENGTH + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9690104d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'and', 'of', 'to', 'that', 'in', 'he', 'shall', 'unto', 'for', 'i', 'his', 'a', 'lord', 'they', 'be', 'is', 'him', 'not', 'them', 'it', 'with', 'all', 'thou', 'thy', 'was', 'god', 'which'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the vocabulary for text dataset\n",
    "vectorize_layer.adapt(dataset.batch(BATCH_SIZE))\n",
    "# Returns a list of all vocabulary tokens sorted (descending) by their frequency\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:30], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b01b6363",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = config_dataset(dataset, buffer_size, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2128843f",
   "metadata": {},
   "source": [
    "## 2.2 The model components\n",
    "    With the dataset ready, it’s time to write the code for the language model itself. We will go for the GPT version of transformer-based models which uses the decoder block from the original transformer model to build a next-token predictor. Causal masking in the multi-head-attention layer allows us to use the entire input sequence as the targets, since embeddings at time point t are only allowed to attent to previous tokens, not subsequent tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b292194e",
   "metadata": {},
   "source": [
    "### 2.2.1 The Embedding\n",
    "    The first thing you want to do is write a subclassed layer class that embeds the individual token indices in the input (each index should be mapped to a vector that is looked up from a table). For this you can use tf.keras.layers.Embedding, in which the input dimension should be the vocabulary size that you chose and the output dimension is the dimensionality of the embeddings (try something between 64 and 256). What this subclassed layer should do is embed not only the token indices but also their position in the input. Transformer based models do not inherently operate on sequences but rather on sets of tokens. This means the model will also need to learn a positional embedding with a second embedding layer that has an input dimension of the sequence length and the same output embedding dimension. Without positional encoding, the order of the input sequence would not affect the model in any way.\n",
    "    • In the init method define the two embeddings for the token indices and their position using tf.keras.layers.Embedding. Assign their input and output dim as described above.\n",
    "    • In the call method first construct a tensor with tf.range from zero to m (the input sequence length, which can be obtained from the input’s shape). This will act as the indices to look up the positional code for each sub-word. Then, feed the token index embedding layer with the input sequence and the positional embedding layer with the newly constructed range tensor. FInally add the two embeddings (like it was done in a ResNet) and return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4fcc01b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two separate embedding layers: one for tokens and one for token index (positions).\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(0, maxlen)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "36e5773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def casual_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, axis=-1), tf.constant([1, 1], dtype=tf.int32)], axis=0\n",
    "    )\n",
    "\n",
    "    return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13e4e4",
   "metadata": {},
   "source": [
    "### 2.2.2 The TransformerBlock layer\n",
    "    The next part is the TransformerBlock. You again should create a subclassed layer for this. In it, create a MultiHeadAttention layer with 2-4 attention heads and a key dimension of the dimensionality used for the embeddings in the previous step. If you’d like to understand what is happening inside the MultiHea\u0002dAttention, refer to the Courseware to see how it can be implemented.\n",
    "    • In the init method you have to instantiate the MHA layer, with the num heads and key dim arguments assigned as described above. Additionally, you also need to instantiate two Dense layers. The first has a ReLU activation and between 32 and 256 units and the second has no activation and again as many units as the dimensionality of the embeddings. You will also want to instantiate two dropout layers with a dropout rate of 0.1. Remember that dropout layers require a training argument in the call method. THis needs to be passed down all the way through higher order layers, down to the call method of this layer. Finally, add two layer-normalization layers, with an epsilon of 1e-6.\n",
    "    • In the call method give the input to the multi-head attention layer as both value and query arguments (internally it will then also be used as the keys), meaning the embedded inputs are used as both the query, value and key arguments. Importantly the use causal mask argument has to be set to true during the call, so the model does not attend to future tokens. Then you use dropout on the output of the MHA and add the result of that back to the layer input, like in residual connections in a ResNet. To the sum, you apply layer normalization. Sometimes, adding the layer norm is done before the sum for stability reasons. The result - let’s call it ln_out - will be used for another residual connection: Apply the two dense layers to it, followed by another dropout layer, and then add ln_out back to the result of those dense and dropout layers (the residual connection). Remember the training argument. Finally, apply the secondlayer normalization and return the final output.\n",
    "    Wow. That was a lot! But the good news is, you’re almost done building a GPT-like text generator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "676980fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Transformer block as a layer\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        # Transformer block multi-head Self Attention\n",
    "        self.multiHeadSelfAtt = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layer_norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout_1 = layers.Dropout(dropout)\n",
    "        self.dropout_2 = layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        casual_mask = casual_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        attention_output = self.multiHeadSelfAtt(inputs, inputs, attention_mask=casual_mask)\n",
    "        attention_output = self.dropout_1(attention_output)\n",
    "        out_1 = self.layer_norm_1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out_1)\n",
    "        ffn_output = self.dropout_2(ffn_output)\n",
    "        \n",
    "        return self.layer_norm_2(out_1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ef6bc3",
   "metadata": {},
   "source": [
    "### 2.2.3 The subclassed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a2c68fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTransformerModel(vocab_size, sequence_length, embed_dim, num_heads, feed_forward_dim):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(sequence_length,), dtype=tf.int32))\n",
    "    # Add a class with two separate embedding layers\n",
    "    model.add(TokenAndPositionEmbedding(sequence_length, num_heads, feed_forward_dim))\n",
    "    model.add(TransformerBlock(embed_dim, num_heads, feed_forward_dim))\n",
    "    model.add(layers.Dense(units=vocab_size))\n",
    "\n",
    "    loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=[loss_function, None]\n",
    "    )\n",
    "    # No loss and optimization based on word embeddings from transformer block\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "72f061eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model.\n",
    "    1. Feed some starting prompt to the model\n",
    "    2. Predict probabilities for the next token\n",
    "    3. Sample the next token and add it to the next input\n",
    "\n",
    "    Arguments:\n",
    "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
    "        start_tokens: List of integers, the token indices for the starting prompt.\n",
    "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
    "        top_k: Integer, sample from the `top_k` token predictions.\n",
    "        print_every: Integer, print after this many epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_tokens, start_tokens, index_to_word, sequence_length, k=10, print_every=1):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.k = k\n",
    "        self.print_every = print_every\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = tf.math.top_k(logits, self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype('int32')\n",
    "        predictions = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        predictions = np.asarray(predictions).astype('float32') \n",
    "\n",
    "        return np.random.choice(indices, p=predictions)\n",
    "    \n",
    "    def detokenize(self, number):\n",
    "        return self.index_to_word[number]\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        \n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = self.sequence_length - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:self.sequence_length]\n",
    "                sample_index = self.sequence_length - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "\n",
    "            x = np.array([x])\n",
    "            y = self.model.predict(x)\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "        \n",
    "        text = \" \".join([self.detokenize(_) for _ in self.start_tokens + tokens_generated]) \n",
    "        print(f'Generated text: {text}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "31123246",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "# Tokenize starting prompt\n",
    "for index, word in enumerate(inverse_vocab):\n",
    "    word_to_index[word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4ae59385",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prompt = 'The First Book'\n",
    "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
    "num_tokens_generated = 50\n",
    "text_generator_callback = TextGenerator(num_tokens_generated, start_tokens, \n",
    "                                        inverse_vocab, SEQUENCE_LENGTH)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "cbce45ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = createTransformerModel(vocab_size=vocab_size, \n",
    "                                    sequence_length=SEQUENCE_LENGTH, \n",
    "                                    embed_dim=embedding_dim,\n",
    "                                    num_heads=NUM_HEADS,\n",
    "                                    feed_forward_dim=FEED_FORWARD_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "265398e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 371ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "Generated text: [UNK] [UNK] [UNK] of  that not the is not that unto  is the  of of of the that not is  unto that the the  of and the and and that that of and and the and that of not and that that unto that the of a the the\n",
      "\n",
      "583/583 [==============================] - 220s 373ms/step - loss: 6.0350\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 40ms/step- loss: 5.47\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "Generated text: [UNK] [UNK] [UNK] the of of of and and unto that of  the of is a  and that unto and of the of the the the unto the the to that to the  to the the unto the that of  of  of of a a the and the to\n",
      "\n",
      "583/583 [==============================] - 309s 531ms/step - loss: 5.4794\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 43ms/step- loss: 5.41\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "Generated text: [UNK] [UNK] [UNK] the and to of that  of  of the and of the the of and is that the unto and that of he the and of a unto and the  a and and and the of a of of to the the he the and that the of of\n",
      "\n",
      "583/583 [==============================] - 305s 524ms/step - loss: 5.4116\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 40ms/step- loss: 5.36\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Generated text: [UNK] [UNK] [UNK] of the and a  of is is and to  and and of that and to of  and of unto is and the is that and  the and the a the and and the the the in that of of and to and to that unto and that\n",
      "\n",
      "583/583 [==============================] - 305s 524ms/step - loss: 5.3694\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 80ms/step- loss: 5.34\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "Generated text: [UNK] [UNK] [UNK] the of  the a and  of the the of  the in of the  the of and unto the and the to of and and  the is that of and the in that the of and the a is and the and and the of that of\n",
      "\n",
      "583/583 [==============================] - 311s 534ms/step - loss: 5.3469\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 174ms/step loss: 5.33\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "Generated text: [UNK] [UNK] [UNK] the unto the of a unto  be that to and and and and is and and the and of and and of unto that of of to of of and of and of is the unto of a that is a of of the the and is of of that\n",
      "\n",
      "583/583 [==============================] - 322s 553ms/step - loss: 5.3374\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 90ms/step- loss: 5.33\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 1s 756ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Generated text: [UNK] [UNK] [UNK]                                                   \n",
      "\n",
      "583/583 [==============================] - 384s 658ms/step - loss: 5.3358\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 50ms/step- loss: 5.33\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Generated text: [UNK] [UNK] [UNK] that and and the and unto and of that of of the be be to to a that   of in unto  the be and and to  unto and that the the the to a and of be the the in the the the be unto that \n",
      "\n",
      "583/583 [==============================] - 315s 541ms/step - loss: 5.3336\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 54ms/step- loss: 5.33\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "Generated text: [UNK] [UNK] [UNK] the that that and of a of of the  be the and of of to the of the that  and be  and and and of the that the that be unto be and be of of and of and  and unto the and the in the the\n",
      "\n",
      "583/583 [==============================] - 311s 533ms/step - loss: 5.3324\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 40ms/step- loss: 5.33\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Generated text: [UNK] [UNK] [UNK] is and to the of in the that and that the and and that and and the in  to of  the the the of and and and a  of and and and the and that and and and the the unto the of to and the of and\n",
      "\n",
      "583/583 [==============================] - 305s 524ms/step - loss: 5.3313\n"
     ]
    }
   ],
   "source": [
    "history = gpt_model.fit(dataset,\n",
    "                        epochs=10,\n",
    "                        verbose=1,\n",
    "                        callbacks=[text_generator_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "93c29c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " token_and_position_embeddin  (None, 4, 64)            384       \n",
      " g_7 (TokenAndPositionEmbedd                                     \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transformer_block_7 (Transf  (None, 4, 64)            41792     \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 4, 72192)          4692480   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,734,656\n",
      "Trainable params: 4,734,656\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gpt_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75471f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
